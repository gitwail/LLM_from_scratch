# LLM_from_scratch

| Path              | Description                                             |
|-------------------|---------------------------------------------------------|
| `README.md`       | Project overview, roadmap, diagrams                     |
| `data/`           | Preprocessing scripts & raw datasets                    |
| `tokenizer/`      | BPE & tokenizer code                                    |
| `model/`          | Attention, transformer, GPT, kv-cache                   |
| `training/`       | Loss, optimizer, precision training                     |
| `finetune/`       | SFT, LoRA, PPO, DPO                                     |
| `inference/`      | Generation, kv-cache, quantization                      |
| `deployment/`     | API, Streamlit app, quantized model                     |
| `scripts/`        | Train / test / evaluate scripts                         |
| `tests/`          | Unit tests for core modules                             |
| `notebooks/`      | Exploratory + visualizations (Colab-ready)             |



---

## üìÖ Weekly Build & Learn Roadmap
> **How to use:**  
> 1. Complete the tasks.  
> 2. Check the box.  
> 3. Commit code + a markdown log (`logs/week_X.md`).  
> Recruiters will see constant, verifiable progress.

### Week‚ÄØ1¬†‚Äî Introduction & Bigram¬†LM
- [ ] Watch Karpathy *‚ÄúLet‚Äôs build GPT ‚Äî start‚Äù*  
- [ ] **Dataset**: Download `tinyshakespeare.txt`
- [ ] **Tokenizer**: char‚Äëlevel vocab (`tokenizer/char.py`)
- [ ] **Bigram Model** (`model/bigram.py`)
- [ ] **Train**: basic loop, log train/val loss
- [ ] **Checkpoint**: explain bigram limits in `logs/week_1.md`
- [ ] **Output**: save generated gibberish sample

---

### Week‚ÄØ2¬†‚Äî MLP¬†+¬†Positional¬†Encoding
- [ ] Continue Karpathy video (MLP section)
- [ ] **MLP¬†Block** (`model/mlp_block.py`)
- [ ] **Learned¬†positional embeddings**
- [ ] **Robust training loop** (`training/train_mlp.py`)
- [ ] **Checkpoint**: compare loss vs Week‚ÄØ1
- [ ] **Output**: text sample, note coherence gains

---

### Week‚ÄØ3¬†‚Äî Causal Self‚ÄëAttention
- [ ] Watch self‚Äëattention part of Karpathy
- [ ] **Single‚Äëhead attention** (`model/attn_head.py`)
- [ ] **Causal mask** with `torch.tril`
- [ ] **Multi‚Äëhead wrapper** (`model/mha.py`)
- [ ] **Visualize** attention weights (notebook)
- [ ] **Checkpoint**: explain why attention > fixed‚Äëwindow MLP

---

### Week‚ÄØ4¬†‚Äî Transformer Block & Mini‚ÄëGPT
- [ ] Finish Karpathy video (Transformer assembly)
- [ ] **Block**: LayerNorm ‚Üí MHA ‚Üí MLP + residuals
- [ ] **Stack** 4‚Äë6 blocks (`model/mini_gpt.py`)
- [ ] **Train** on Shakespeare
- [ ] **Checkpoint**: record hyperparams & loss curve
- [ ] **Output**: Shakespeare‚Äëlike generation sample

---

### Week‚ÄØ5¬†‚Äî Byte‚ÄëPair‚ÄØEncoding Tokenizer
- [ ] Watch *‚ÄúLet‚Äôs build the GPT Tokenizer‚Äù*
- [ ] **BPE** implementation (`tokenizer/bpe.py`)
- [ ] **Encode/Decode** round‚Äëtrip tests
- [ ] **Swap in** BPE tokens for training
- [ ] **Checkpoint**: char vs BPE length comparison
- [ ] **Output**: demo encoding table

---

### Week‚ÄØ6¬†‚Äî Scaling Up: Reproduce GPT‚Äë2‚ÄØ(124‚ÄØM)
- [ ] Watch *‚ÄúLet‚Äôs reproduce GPT‚Äë2 (124M)‚Äù*
- [ ] **Model config**: 12‚ÄØlayers, 768‚ÄØd, 12‚ÄØheads
- [ ] **Data**: OpenWebText/Wikipedia subset
- [ ] **Tricks**: gradient¬†checkpointing, AdamW
- [ ] **Train / or partial‚Äëtrain** (`training/train_gpt2.py`)
- [ ] **Checkpoint**: loss, perplexity, hardware notes
- [ ] **Output**: multi‚Äëparagraph generation

---

### Week‚ÄØ7¬†‚Äî Mixed Precision (AMP)
- [ ] Study FP32¬†vs¬†FP16/BF16
- [ ] Integrate `torch.cuda.amp`
- [ ] Compare memory + speed
- [ ] **Checkpoint**: metrics table
- [ ] **Output**: screenshot/log of GPU util drop

---

### Week‚ÄØ8¬†‚Äî Quantization for Inference
- [ ] Learn int8 / 4‚Äëbit quantization basics
- [ ] Apply `bitsandbytes` or `torch.quantize_dynamic`
- [ ] Measure size & latency
- [ ] **Checkpoint**: pre/post size report
- [ ] **Output**: text sample from quantized model

---

### Week‚ÄØ9¬†‚Äî PEFT: LoRA / QLoRA Fine‚ÄëTuning
- [ ] Read PEFT docs, LoRA paper
- [ ] Fine‚Äëtune on custom dataset (`finetune/lora.py`)
- [ ] **Checkpoint**: params updated vs total
- [ ] **Output**: before‚Äëvs‚Äëafter response demo

---

### Week‚ÄØ10¬†‚Äî Alignment: RLHF /¬†DPO¬†(Mini)
- [ ] Skim RLHF & DPO overviews
- [ ] Implement toy reward + PPO loop (`finetune/ppo.py`)
- [ ] **Checkpoint**: reward curve
- [ ] **Output**: behavioral change example

---

### Week‚ÄØ11¬†‚Äî Retrieval‚ÄëAugmented Generation (RAG)
- [ ] Build FAISS vector store (`inference/rag.py`)
- [ ] Pipeline: retrieve ‚Üí prompt ‚Üí generate
- [ ] **Checkpoint**: accuracy with/without retrieval
- [ ] **Output**: illustrated Q&A example

---

### Week‚ÄØ12¬†‚Äî Deployment & Demo
- [ ] Export quantized model
- [ ] Build FastAPI endpoints (`deployment/api.py`)
- [ ] Create Streamlit / Gradio UI
- [ ] Dockerfile + GitHub¬†Actions CI/CD
- [ ] **Checkpoint**: live demo link
- [ ] **Output**: 2‚Äëmin video walkthrough

---

## üèÅ Deliverables Checklist
- [ ] Loss/perplexity plots (PNG in `reports/`)
- [ ] Attention heatmaps (Week¬†3)
- [ ] Model cards (base, fine‚Äëtuned, quantized)
- [ ] Blog posts (Medium / Hashnode) linked here
- [ ] Demo video + LinkedIn announcement
- [ ] Recruiter‚Äëfriendly summary in `SUMMARY.md`

---

## üîó Key Resources
- Karpathy *LLM‚ÄØZero‚Äëto‚ÄëHero* playlist  
- Stanford **CS224n** lectures & assignments  
- Sebastian¬†Raschka *LLMs‚Äëfrom‚Äëscratch* repo  

> **Clone & start:**  
> ```bash
> git clone https://github.com/<your_handle>/llm-from-scratch.git
> cd llm-from-scratch && pip install -r requirements.txt
> python scripts/download_data.py
> python training/train_bigram.py
> ```

---

*Built with ‚ù§Ô∏è & curiosity ‚Äî track my progress on the `progress` branch!*  
