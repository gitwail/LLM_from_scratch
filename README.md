# LLM_from_scratch

llm-from-scratch/
â”‚
â”œâ”€â”€ README.md                        <- Project overview, roadmap, diagrams
â”œâ”€â”€ data/                            <- Preprocessing scripts & raw datasets
â”œâ”€â”€ tokenizer/                       <- BPE & tokenizer code
â”œâ”€â”€ model/                           <- Attention, transformer, GPT, kv-cache
â”œâ”€â”€ training/                        <- Loss, optimizer, precision training
â”œâ”€â”€ finetune/                        <- SFT, LoRA, PPO, DPO
â”œâ”€â”€ inference/                       <- Generation, kv-cache, quantization
â”œâ”€â”€ deployment/                      <- API, Streamlit app, quantized model
â”œâ”€â”€ scripts/                         <- Train / test / evaluate scripts
â”œâ”€â”€ tests/                           <- Unit tests for core modules
â””â”€â”€ notebooks/                       <- Exploratory + visualizations (Colab-ready)


---

## ğŸ“… Weekly Build & Learn Roadmap
> **How to use:**  
> 1. Complete the tasks.  
> 2. Check the box.  
> 3. Commit code + a markdown log (`logs/week_X.md`).  
> Recruiters will see constant, verifiable progress.

### Weekâ€¯1Â â€” Introduction & BigramÂ LM
- [ ] Watch Karpathy *â€œLetâ€™s build GPT â€” startâ€*  
- [ ] **Dataset**: Download `tinyshakespeare.txt`
- [ ] **Tokenizer**: charâ€‘level vocab (`tokenizer/char.py`)
- [ ] **Bigram Model** (`model/bigram.py`)
- [ ] **Train**: basic loop, log train/val loss
- [ ] **Checkpoint**: explain bigram limits in `logs/week_1.md`
- [ ] **Output**: save generated gibberish sample

---

### Weekâ€¯2Â â€” MLPÂ +Â PositionalÂ Encoding
- [ ] Continue Karpathy video (MLP section)
- [ ] **MLPÂ Block** (`model/mlp_block.py`)
- [ ] **LearnedÂ positional embeddings**
- [ ] **Robust training loop** (`training/train_mlp.py`)
- [ ] **Checkpoint**: compare loss vs Weekâ€¯1
- [ ] **Output**: text sample, note coherence gains

---

### Weekâ€¯3Â â€” Causal Selfâ€‘Attention
- [ ] Watch selfâ€‘attention part of Karpathy
- [ ] **Singleâ€‘head attention** (`model/attn_head.py`)
- [ ] **Causal mask** with `torch.tril`
- [ ] **Multiâ€‘head wrapper** (`model/mha.py`)
- [ ] **Visualize** attention weights (notebook)
- [ ] **Checkpoint**: explain why attention > fixedâ€‘window MLP

---

### Weekâ€¯4Â â€” Transformer Block & Miniâ€‘GPT
- [ ] Finish Karpathy video (Transformer assembly)
- [ ] **Block**: LayerNorm â†’ MHA â†’ MLP + residuals
- [ ] **Stack** 4â€‘6 blocks (`model/mini_gpt.py`)
- [ ] **Train** on Shakespeare
- [ ] **Checkpoint**: record hyperparams & loss curve
- [ ] **Output**: Shakespeareâ€‘like generation sample

---

### Weekâ€¯5Â â€” Byteâ€‘Pairâ€¯Encoding Tokenizer
- [ ] Watch *â€œLetâ€™s build the GPT Tokenizerâ€*
- [ ] **BPE** implementation (`tokenizer/bpe.py`)
- [ ] **Encode/Decode** roundâ€‘trip tests
- [ ] **Swap in** BPE tokens for training
- [ ] **Checkpoint**: char vs BPE length comparison
- [ ] **Output**: demo encoding table

---

### Weekâ€¯6Â â€” Scaling Up: Reproduce GPTâ€‘2â€¯(124â€¯M)
- [ ] Watch *â€œLetâ€™s reproduce GPTâ€‘2 (124M)â€*
- [ ] **Model config**: 12â€¯layers, 768â€¯d, 12â€¯heads
- [ ] **Data**: OpenWebText/Wikipedia subset
- [ ] **Tricks**: gradientÂ checkpointing, AdamW
- [ ] **Train / or partialâ€‘train** (`training/train_gpt2.py`)
- [ ] **Checkpoint**: loss, perplexity, hardware notes
- [ ] **Output**: multiâ€‘paragraph generation

---

### Weekâ€¯7Â â€” Mixed Precision (AMP)
- [ ] Study FP32Â vsÂ FP16/BF16
- [ ] Integrate `torch.cuda.amp`
- [ ] Compare memory + speed
- [ ] **Checkpoint**: metrics table
- [ ] **Output**: screenshot/log of GPU util drop

---

### Weekâ€¯8Â â€” Quantization for Inference
- [ ] Learn int8 / 4â€‘bit quantization basics
- [ ] Apply `bitsandbytes` or `torch.quantize_dynamic`
- [ ] Measure size & latency
- [ ] **Checkpoint**: pre/post size report
- [ ] **Output**: text sample from quantized model

---

### Weekâ€¯9Â â€” PEFT: LoRA / QLoRA Fineâ€‘Tuning
- [ ] Read PEFT docs, LoRA paper
- [ ] Fineâ€‘tune on custom dataset (`finetune/lora.py`)
- [ ] **Checkpoint**: params updated vs total
- [ ] **Output**: beforeâ€‘vsâ€‘after response demo

---

### Weekâ€¯10Â â€” Alignment: RLHF /Â DPOÂ (Mini)
- [ ] Skim RLHF & DPO overviews
- [ ] Implement toy reward + PPO loop (`finetune/ppo.py`)
- [ ] **Checkpoint**: reward curve
- [ ] **Output**: behavioral change example

---

### Weekâ€¯11Â â€” Retrievalâ€‘Augmented Generation (RAG)
- [ ] Build FAISS vector store (`inference/rag.py`)
- [ ] Pipeline: retrieve â†’ prompt â†’ generate
- [ ] **Checkpoint**: accuracy with/without retrieval
- [ ] **Output**: illustrated Q&A example

---

### Weekâ€¯12Â â€” Deployment & Demo
- [ ] Export quantized model
- [ ] Build FastAPI endpoints (`deployment/api.py`)
- [ ] Create Streamlit / Gradio UI
- [ ] Dockerfile + GitHubÂ Actions CI/CD
- [ ] **Checkpoint**: live demo link
- [ ] **Output**: 2â€‘min video walkthrough

---

## ğŸ Deliverables Checklist
- [ ] Loss/perplexity plots (PNG in `reports/`)
- [ ] Attention heatmaps (WeekÂ 3)
- [ ] Model cards (base, fineâ€‘tuned, quantized)
- [ ] Blog posts (Medium / Hashnode) linked here
- [ ] Demo video + LinkedIn announcement
- [ ] Recruiterâ€‘friendly summary in `SUMMARY.md`

---

## ğŸ”— Key Resources
- Karpathy *LLMâ€¯Zeroâ€‘toâ€‘Hero* playlist  
- Stanford **CS224n** lectures & assignments  
- SebastianÂ Raschka *LLMsâ€‘fromâ€‘scratch* repo  

> **Clone & start:**  
> ```bash
> git clone https://github.com/<your_handle>/llm-from-scratch.git
> cd llm-from-scratch && pip install -r requirements.txt
> python scripts/download_data.py
> python training/train_bigram.py
> ```

---

*Built with â¤ï¸ & curiosity â€” track my progress on the `progress` branch!*  
